{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __FW-to-BIDS__\n",
    "\n",
    "## Download new flywheel sessions and convert them into BIDs format \n",
    "\n",
    "### Needs the following user input: \n",
    "#### 1) project_name: the project name\n",
    "\n",
    "#### 2) bids_dir: the BIDS_dir you are adding to\n",
    "\n",
    "#### 3) skip_complete: a boolean indicating whether so skip already converted files (True) or convert all files (False)\n",
    "\n",
    "### __Requires 1) pydeface to be installed & 2) to be run in python3__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for fw downloading\n",
    "import flywheel\n",
    "\n",
    "#for both downloading and converting to BIDS\n",
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "\n",
    "#for BIDS conversion\n",
    "import json\n",
    "import nibabel as nib\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __User Input:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_complete = True\n",
    "\n",
    "project_name = 'russpold/uh2aim4'\n",
    "bids_dir = 'uh2aim4_bids'\n",
    "\n",
    "#if the bids_dir doesn't exist, make it and write a blank README\n",
    "if not os.path.exists(bids_dir):\n",
    "    os.mkdir(bids_dir)\n",
    "    open(bids_dir+'/README', 'a').close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Part I Download New Sessions from Flywheel__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open flywheel and get the current project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = flywheel.Client()\n",
    "\n",
    "\n",
    "project = fw.lookup(project_name)\n",
    "uh2aim4_id = project.id # find correct project id \n",
    "\n",
    "fw.get_current_user() #Shows your current login situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sessions = glob.glob(os.path.join(bids_dir, '*/*'))\n",
    "local_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list/dictionary of subject IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_dict = {} \n",
    "ids=[]\n",
    "for subject in project.subjects():\n",
    "    id_dict.update({subject.label: subject.id})\n",
    "    ids.append(subject.label)\n",
    "    for session in subject.sessions():\n",
    "        print('%s: %s' % (session.id, session.label))\n",
    "    \n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make or update participants.tsv as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ids, columns=['participant_ids'])\n",
    "df.to_csv(bids_dir+'/participants.tsv',index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare flywheel sessions to local & find the new sessions to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sessions = []\n",
    "\n",
    "for subject in project.subjects():\n",
    "    fw_subj_sessions = subject.sessions()\n",
    "    local_subj_sessions = glob.glob(os.path.join(bids_dir, 'sub-%s' % subject.label)) #CHANGE TO BIDS ONCE SCRIPT IS UP AND RUNNING  \n",
    "    num_missing = len(fw_subj_sessions) - len(local_subj_sessions)\n",
    "    if num_missing > 0:\n",
    "        for i in range(num_missing):\n",
    "            new_sessions.append(fw_subj_sessions[i])\n",
    "            \n",
    "            \n",
    "            \n",
    "print('new session(s): ')\n",
    "for ses in new_sessions:\n",
    "    print(ses.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the new sessions as individual tar files\n",
    "This can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_ses in new_sessions:\n",
    "    print('downloading new-session-%s-%s.tar' % (new_ses.subject.code, new_ses.label))\n",
    "    fw.download_tar([new_ses], 'new-session-%s-%s.tar' % (new_ses.subject.code, new_ses.label), exclude_types=['dicom', 'pfile'])\n",
    "    \n",
    "print('downloading complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Part II - Extract & Organize files into BIDS__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_ses in new_sessions:\n",
    "    print('extracting new-session-%s-%s.tar' % (new_ses.subject.code, new_ses.label))\n",
    "    tar = tarfile.open('new-session-%s-%s.tar' % (new_ses.subject.code, new_ses.label))\n",
    "    tar.extractall()\n",
    "    \n",
    "print('extraction complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardiac_bids = {\n",
    "    \"StartTime\": -30.0,\n",
    "    \"SamplingFrequency\": .01,\n",
    "    \"Columns\": [\"cardiac\"]\n",
    "}\n",
    "\n",
    "respiratory_bids = {\n",
    "    \"StartTime\": -30.0,\n",
    "    \"SamplingFrequency\":  .04,\n",
    "    \"Columns\": [\"respiratory\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizes file names\n",
    "def clean_file(f):\n",
    "    f = f.replace('task_', 'task-').replace('run_','run-').replace('_ssg','')\n",
    "    return f\n",
    "#renames files with standardized file names\n",
    "def cleanup(path):\n",
    "    for f in glob.glob(os.path.join(path, '*')):\n",
    "        new_name = clean_file(f)\n",
    "        os.rename(f,new_name)\n",
    "        \n",
    "        \n",
    "def bids_anat(sub_id, anat_dir, anat_path):\n",
    "    \"\"\"\n",
    "    Moves and converts a anatomical folder associated with a T1 or T2\n",
    "    to BIDS format. Assumes files are organized appropriately\n",
    "    (e.g. \"project_data/s999/ses-1/anat-[T1w,T2w]\")\n",
    "    \"\"\"\n",
    "    #T1 or T2?\n",
    "    path_pieces = anat_dir.split('/')\n",
    "    anat_type = [x for x in path_pieces if 'anat' in x][0].split('-')[1]\n",
    "    anat_file = glob.glob(os.path.join(anat_dir,'*nii.gz'))\n",
    "    assert len(anat_file) == 1, \"More than one anat file found in directory %s\" % anat_dir\n",
    "    new_file = os.path.join(anat_path, sub_id + '_' + anat_type + '.nii.gz')\n",
    "    if not os.path.exists(new_file):\n",
    "        to_write.write('\\tDefacing...\\n')\n",
    "        subprocess.call(\"pydeface %s --outfile %s\" % (anat_file[0], new_file), shell=True)\n",
    "    else:\n",
    "        to_write.write('Did not save anat because %s already exists!\\n' % new_file)        \n",
    "\n",
    "\n",
    "def bids_fmap(base_file_id, fmap_dir, fmap_path, func_path):\n",
    "    \"\"\"\n",
    "    Moves and converts an epi folder associated with a fieldmap\n",
    "    to BIDS format. Assumes files are organized appropriately\n",
    "    (e.g. \"project_data/s999/ses-1/fmap-fieldmap/[examinfo]_fieldmap.jason\"\n",
    "          \"project_data/s999/ses-1/fmap-fieldmap/[examinfo]_fieldmap.nii.gz\" )\n",
    "    \"\"\"\n",
    "    fmap_files = glob.glob(os.path.join(fmap_dir,'*.nii.gz'))\n",
    "    assert len(fmap_files) == 2, \"Didn't find the correct number of files in %s\" % fmap_dir\n",
    "    fmap_index = [0,1]['fieldmap.nii.gz' in fmap_files[1]]\n",
    "    fmap_file = fmap_files[fmap_index]\n",
    "    mag_file = fmap_files[1-fmap_index]\n",
    "    fmap_name = base_file_id + '_' + 'fieldmap.nii.gz'\n",
    "    mag_name = base_file_id + '_' + 'magnitude.nii.gz'\n",
    "    if not os.path.exists(os.path.join(fmap_path, fmap_name)):\n",
    "        shutil.copyfile(fmap_file, os.path.join(fmap_path, fmap_name))\n",
    "        shutil.copyfile(mag_file, os.path.join(fmap_path, mag_name))\n",
    "        func_runs = [os.sep.join(os.path.normpath(f).split(os.sep)[-3:]) for f in glob.glob(os.path.join(func_path,'*task*bold.nii.gz'))]\n",
    "        fieldmap_meta = {'Units': 'Hz', 'IntendedFor': func_runs}\n",
    "        json.dump(fieldmap_meta,open(os.path.join(fmap_path, base_file_id + '_fieldmap.json'),'w'))\n",
    "    else:\n",
    "        to_write.write('Did not save fmap_epi because %s already exists!\\n' % os.path.join(fmap_path, fmap_name))\n",
    "\n",
    "#bids_sbref(base_file_id, sbref_dir, func_path, bids_dir)        \n",
    "def bids_sbref(base_file_id, sbref_dir, func_path, bids_dir):\n",
    "    \"\"\"\n",
    "    Moves and converts an epi folder associated with a sbref\n",
    "    calibration scan to BIDS format. Assumes tasks are organized appropriately\n",
    "    (e.g. \"project_data/s999/ses-1/task-rest_run-1_sbref/[examinfo].json\"\n",
    "          \"project_data/s999/ses-1/task-rest_run-1_sbref/[examinfo].nii.gz\" )    \n",
    "    \"\"\"\n",
    "    filename = '_'.join(os.path.basename(sbref_dir).split('_'))\n",
    "    sbref_files = glob.glob(os.path.join(sbref_dir,'*.nii.gz'))\n",
    "    # remove files that are sometimes added, but are of no interest\n",
    "    sbref_files = [i for i in sbref_files if 'phase' not in i]\n",
    "    assert len(sbref_files) <= 1, \"More than one func file found in directory %s\" % sbref_dir\n",
    "    if len(sbref_files) == 0:\n",
    "        to_write.write('Skipping %s, no nii.gz file found\\n' % sbref_dir)\n",
    "        return\n",
    "\n",
    "    # bring to subject directory and divide into sbref and bold\n",
    "    sbref_file = clean_file(os.path.join(func_path, base_file_id + '_' + filename + '.nii.gz'))\n",
    "    # check if file exists. If it does, check if the saved file has more time points\n",
    "    if os.path.exists(sbref_file):\n",
    "        to_write.write('%s already exists!\\n' % sbref_file)\n",
    "        saved_shape = nib.load(sbref_file).shape\n",
    "        current_shape = nib.load(sbref_files[0]).shape\n",
    "        to_write.write('Dimensions of saved image: %s\\n' % list(saved_shape))\n",
    "        to_write.write('Dimensions of current image: %s\\n' % list(current_shape))\n",
    "        if (current_shape[-1] <= saved_shape[-1]):\n",
    "            to_write.write('Current image has fewer or equivalent time points than saved image. Exiting...\\n')\n",
    "            return\n",
    "        else:\n",
    "            to_write.write('Current image has more time points than saved image. Overwriting...\\n')\n",
    "    # save sbref image to bids directory\n",
    "    shutil.copyfile(sbref_files[0], sbref_file)\n",
    "    # get metadata\n",
    "    sbref_meta_path = clean_file(os.path.join(bids_dir, re.sub('_run[-_][0-9]','',filename) + '.json'))\n",
    "    if not os.path.exists(sbref_meta_path):\n",
    "        try:\n",
    "            json_file = [x for x in glob.glob(os.path.join(sbref_dir,'*.json')) \n",
    "                            if 'qa' not in x][0]\n",
    "            func_meta = get_functional_meta(json_file, filename)\n",
    "            json.dump(func_meta,open(sbref_meta_path,'w'))\n",
    "        except IndexError:\n",
    "            to_write.write(\"Metadata couldn't be created for %s\\n\" % sbref_file)        \n",
    "\n",
    "            \n",
    "def task_filter(task_dirs):\n",
    "    \"\"\"\n",
    "    Filters out all but the newest run of a task with duplicates\n",
    "    \"\"\"\n",
    "    out_dirs = task_dirs\n",
    "    for task in sorted(task_dirs):\n",
    "        tasks = [x for x in task_dirs if task in x]\n",
    "        if len(tasks) > 1:\n",
    "            tasks = sorted(tasks, reverse=True)\n",
    "            to_be_removed = tasks[1:]\n",
    "            out_dirs = [x for x in out_dirs if x not in to_be_removed]\n",
    "    return(out_dirs)\n",
    "\n",
    "#bids_task(base_file_id, task_dir, func_path, bids_dir)\n",
    "def bids_task(base_file_id, task_dir, func_path, bids_dir):\n",
    "    \"\"\"\n",
    "    Moves and converts an epi folder associated with a task\n",
    "    to BIDS format. Assumes tasks are organized appropriately\n",
    "    (e.g. \"project_data/s999/ses-1/task-stopSignal_run-1_ssg/[examinfo].json\"\n",
    "          \"project_data/s999/ses-1/task-stopSignal_run-1_ssg/[examinfo].nii.gz\" ) \n",
    "    \"\"\"\n",
    "    taskname = os.path.basename(task_dir)\n",
    "    task_file = [f for f in glob.glob(os.path.join(task_dir,'*1.nii.gz')) if \"fieldmap\" not in f]\n",
    "    assert len(task_file) <= 1, \"More than one func file found in directory %s\" % task_dir\n",
    "    if len(task_file) == 0:\n",
    "        to_write.write('Skipping %s, no nii.gz file found\\n' % task_dir)\n",
    "        return\n",
    "\n",
    "    # bring to subject directory and divide into sbref and bold\n",
    "    \n",
    "    bold_file = os.path.join(func_path, base_file_id + '_' + taskname + '.nii.gz')\n",
    "    bold_file = bold_file.replace('_ssg*', '_bold')\n",
    "    # check if file exists. If it does, check if the saved file has more time points\n",
    "    if os.path.exists(bold_file):\n",
    "        to_write.write('%s already exists!\\n' % bold_file)\n",
    "        saved_shape = nib.load(bold_file).shape\n",
    "        current_shape = nib.load(task_file[0]).shape\n",
    "        to_write.write('Dimensions of saved image: %s\\n' % list(saved_shape))\n",
    "        to_write.write('Dimensions of current image: %s\\n' % list(current_shape))\n",
    "        if (current_shape[-1] <= saved_shape[-1]):\n",
    "            to_write.write('Current image has fewer or equal time points than saved image. Exiting...\\n')\n",
    "            return\n",
    "        else:\n",
    "            to_write.write('Current image has more time points than saved image. Overwriting...\\n')\n",
    "    # save bold image to bids directory\n",
    "    shutil.copyfile(task_file[0], bold_file)\n",
    "    # get epi metadata\n",
    "    taskname_pieces = taskname.split('_')\n",
    "    bold_meta_path = os.path.join(bids_dir, re.sub('_run[-_][0-9]','',taskname_pieces[0]) + '_bold.json')\n",
    "    bold_meta_path = clean_file(bold_meta_path)\n",
    "    if not os.path.exists(bold_meta_path):\n",
    "        meta_file = [x for x in glob.glob(os.path.join(task_dir,'*.json')) if 'qa' not in x][0]\n",
    "        func_meta = get_functional_meta(meta_file, taskname)\n",
    "        json.dump(func_meta,open(bold_meta_path,'w'))\n",
    "    # get physio if it exists\n",
    "    physio_file = glob.glob(os.path.join(task_dir, '*physio.zip'))\n",
    "    if len(physio_file)>0:\n",
    "        assert len(physio_file)==1, (\"More than one physio file found in directory %s\" % task_dir)\n",
    "        with ZipFile(physio_file[0], 'r') as zipf:\n",
    "            zipf.extractall(path=func_path)\n",
    "        # extract the actual filename of the physio data\n",
    "        physio_file = os.path.basename(physio_file[0])[:-4]\n",
    "        for pfile in glob.iglob(os.path.join(func_path, physio_file, '*Data*')):\n",
    "            pname = 'respiratory' if 'RESP' in pfile else 'cardiac'\n",
    "            new_physio_file = bold_file.replace('_bold.nii.gz', \n",
    "                                '_recording-' + pname + '_physio.tsv.gz')\n",
    "            f = np.loadtxt(pfile)\n",
    "            np.savetxt(new_physio_file, f, delimiter = '\\t')\n",
    "        shutil.rmtree(os.path.join(func_path, physio_file))\n",
    "\n",
    "# func_meta = get_functional_meta(meta_file, filename)        \n",
    "def get_functional_meta(json_file, taskname):\n",
    "    \"\"\"\n",
    "    Returns BIDS meta data for bold using JSON file\n",
    "    \"\"\"\n",
    "    meta_file = json.load(open(json_file,'r'))\n",
    "    meta_data = {}\n",
    "    mux = meta_file['num_bands']\n",
    "    nslices = meta_file['num_slices'] * mux\n",
    "    tr = meta_file['tr']\n",
    "    n_echoes = meta_file['acquisition_matrix'][0] \n",
    "\n",
    "    # fill in metadata\n",
    "    meta_data['TaskName'] = taskname.split('_')[0]\n",
    "    meta_data['EffectiveEchoSpacing'] = meta_file['effective_echo_spacing']\n",
    "    meta_data['EchoTime'] = meta_file['te']\n",
    "    meta_data['FlipAngle'] = meta_file['flip_angle']\n",
    "    meta_data['RepetitionTime'] = tr\n",
    "    # slice timing\n",
    "    meta_data['SliceTiming'] = meta_file['slice_timing']\n",
    "    total_time = (n_echoes-1)*meta_data['EffectiveEchoSpacing']\n",
    "    meta_data['TotalReadoutTime'] = total_time\n",
    "    meta_data['PhaseEncodingDirection'] = ['i','j','k'][meta_file['phase_encode_direction']] + '-'        \n",
    "    return meta_data\n",
    "\n",
    "\n",
    "def get_session_num(fly_path, sub_id, bids_dir):\n",
    "    '''\n",
    "    takes in a flywheel path the current sub ID and the bids directory\n",
    "    and figures out which session # the current session would take\n",
    "    (e.g. '10099' -> 'ses-2' if it's the subject's second session)\n",
    "    '''\n",
    "    curr_bids_sessions = glob.glob(bids_dir + '/' + sub_id + '/*')\n",
    "    base_ses_num = len(curr_bids_sessions) + 1\n",
    "    curr_session = fly_path.split('/')[4]\n",
    "    curr_fly_sessions = [path.split('/')[4] for path in glob.glob(fly_dir + '/' + sub_id + '/*')]\n",
    "    curr_fly_sessions.sort()\n",
    "    session_num = base_ses_num + curr_fly_sessions.index(curr_session)\n",
    "    return('ses-%d' % session_num)\n",
    "    \n",
    "\n",
    "def get_subj_path(fly_path, bids_dir,):\n",
    "    \"\"\"\n",
    "    Takes a flywheel path and returns a subject id and session #\n",
    "    for an appropriate BIDS path.\n",
    "    DOES NOT CURRENTLY MAKE USE OF id_correction_dict\n",
    "    \"\"\"\n",
    "    path_pieces = fly_path.split('/')\n",
    "    sub_id = path_pieces[3]\n",
    "    session = path_pieces[4]\n",
    "    if '-' in session:\n",
    "        subj_path = os.path.join(bids_dir, 'sub-'+sub_id, session)\n",
    "    else:\n",
    "        subj_path = os.path.join(bids_dir, 'sub-'+sub_id, get_session_num(fly_path, sub_id, bids_dir))\n",
    "    return subj_path\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        to_write.write('Directory %s already existed\\n' % path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Main BIDS Converter Function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bids_subj(subj_path, bids_dir, fly_path, skip_complete=False):\n",
    "    \"\"\"\n",
    "    Takes a subject path (the BIDS path to the subject directory),\n",
    "    a data path (the path to the BIDS directory), and a \n",
    "    fly_path (the path to the subject's data in the original format) \n",
    "    and moves/converts that subject's data to BIDS\n",
    "    \"\"\"\n",
    "    if os.path.exists(subj_path) and skip_complete:\n",
    "        to_write.write(\"Path %s already exists. Skipping.\\n\\n\" % subj_path)\n",
    "    else:\n",
    "        to_write.write(\"********************************************\\n\")\n",
    "        to_write.write(\"BIDSifying %s\\n\" % subj_path)\n",
    "        to_write.write(\"Using flywheel path: %s\\n\" % fly_path)\n",
    "        to_write.write(\"********************************************\\n\\n\")\n",
    "        # extract subject ID\n",
    "        split_path = os.path.normpath(subj_path).split(os.sep)\n",
    "        sub_id = [x for x in split_path if 'sub' in x][0]\n",
    "        ses_id = [x for x in split_path if 'ses' in x][0]\n",
    "        base_file_id = sub_id + '_' + ses_id\n",
    "        # split subject path into a super subject path and a session path\n",
    "        session_path = subj_path\n",
    "        subj_path = os.path.split(subj_path)[0]\n",
    "        mkdir(subj_path)\n",
    "        mkdir(session_path)\n",
    "        anat_path = mkdir(os.path.join(session_path,'anat'))\n",
    "        func_path = mkdir(os.path.join(session_path,'func'))\n",
    "        fmap_path = mkdir(os.path.join(session_path,'fmap'))\n",
    "        # strip paths for rsync transfer\n",
    "        stripped_anat_path = os.sep.join(anat_path.split(os.sep)[-3:-1])\n",
    "        stripped_func_path = os.sep.join(func_path.split(os.sep)[-3:-1])\n",
    "        stripped_fmap_path = os.sep.join(fmap_path.split(os.sep)[-3:-1])\n",
    "\n",
    "        # anat files        \n",
    "        to_write.write(anat_path)\n",
    "        to_write.write('\\nBIDSifying anatomy...\\n')\n",
    "\n",
    "        anat_dirs = sorted(glob.glob(os.path.join(fly_path,'*anat*')))[::-1]\n",
    "        for anat_dir in anat_dirs:\n",
    "            to_write.write('\\t' + anat_dir + '\\n')\n",
    "            bids_anat(base_file_id, anat_dir, anat_path)\n",
    "\n",
    "        # sbref files\n",
    "        to_write.write('\\nBIDSifying sbref...\\n')\n",
    "\n",
    "        sbref_dirs = sorted(glob.glob(os.path.join(fly_path,'*sbref*')))[::-1]\n",
    "        for sbref_dir in sbref_dirs:\n",
    "            to_write.write('\\t' + sbref_dir + '\\n')\n",
    "            bids_sbref(base_file_id, sbref_dir, func_path, bids_dir)\n",
    "\n",
    "        # task files        \n",
    "        to_write.write('\\nBIDSifying task...\\n')\n",
    "\n",
    "        task_dirs = sorted(glob.glob(os.path.join(fly_path,'*task*')))\n",
    "        task_dirs = task_filter(task_dirs)\n",
    "        for task_dir in [x for x in task_dirs if 'sbref' not in x]:\n",
    "            to_write.write('\\t' + task_dir + '\\n')\n",
    "            bids_task(base_file_id, task_dir, func_path, bids_dir)\n",
    "\n",
    "        # cleanup\n",
    "        cleanup(func_path)\n",
    "\n",
    "        # fmap files\n",
    "        to_write.write('\\nBIDSifying fmap...\\n')\n",
    "\n",
    "        fmap_dirs = sorted(glob.glob(os.path.join(fly_path,'*fieldmap*')))[::-1]\n",
    "        for fmap_dir in fmap_dirs:\n",
    "            to_write.write('\\t' + fmap_dir + '\\n')\n",
    "            bids_fmap(base_file_id, fmap_dir, fmap_path, func_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Convert fw files to BIDS format__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants generated by the Script\n",
    "These should not need to be messed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_dir = os.path.join('scitran', project_name)\n",
    "fly_paths = glob.glob(os.path.join(fly_dir, '*/*'))\n",
    "study_id = fly_dir.strip(os.sep).split(os.sep)[2]\n",
    "\n",
    "write_out = '%s_FLY_to_BIDS.txt' % study_id\n",
    "to_write = open(write_out, 'w')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "header = {'Name': study_id, 'BIDSVersion': '1.1-rc1'}\n",
    "json.dump(header,open(os.path.join(bids_dir, 'dataset_description.json'),'w'))\n",
    "error_file = os.path.join(bids_dir, 'error_record.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each new session, convert to BIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fly_path in enumerate(sorted(fly_paths)):\n",
    "    to_write.write(\"BIDSifying path %s out of %s\\n\" % (str(i+1), str(len(fly_paths))))\n",
    "    subj_path  = get_subj_path(fly_path, bids_dir)\n",
    "    print('Flywheel path:', fly_path)\n",
    "    print('Subject Path: ', subj_path)\n",
    "    if subj_path == None:\n",
    "        to_write.write(\"Couldn't find subj_path for %s\\n\" % fly_path)\n",
    "        with open(error_file, 'a') as filey:\n",
    "            filey.write(\"Couldn't find subj_path for %s\\n\" % fly_path)\n",
    "        continue\n",
    "    bids_subj(subj_path, bids_dir, fly_path, skip_complete)\n",
    "    to_write.flush()\n",
    "\n",
    "# add physio metadata\n",
    "if not os.path.exists(os.path.join(bids_dir, 'recording-cardiac_physio.json')):\n",
    "    if len(glob.glob(os.path.join(bids_dir, 'sub-*', 'ses-*', 'func', '*cardiac*'))) > 0:\n",
    "        json.dump(cardiac_bids, open(os.path.join(bids_dir, 'recording-cardiac_physio.json'),'w'))\n",
    "if not os.path.exists(os.path.join(bids_dir, 'recording-respiratory_physio.json')):\n",
    "    if len(glob.glob(os.path.join(bids_dir, 'sub-*', 'ses-*', 'func', '*respiratory*'))) > 0:\n",
    "        json.dump(respiratory_bids, open(os.path.join(bids_dir, 'recording-respiratory_physio.json'),'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housekeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleanup file names and close write object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup(bids_dir)\n",
    "to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_ses in new_sessions:\n",
    "    os.remove('new-session-%s-%s.tar' % (new_ses.subject.code, new_ses.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Flywheel Directory (extracted tar files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('scitran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly_path = fly_paths[0]\n",
    "subj_path= get_subj_path(fly_path, bids_dir)\n",
    "task_dirs = sorted(glob.glob(os.path.join(fly_path,'*task*')))\n",
    "task_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
